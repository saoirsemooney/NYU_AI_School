# -*- coding: utf-8 -*-
"""Copy of Lab 1: Regression (advanced; student).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G_-CRgkduaNT9W0a0dnH8biQnp1BmUxz

**NOTE**: Please make a personal copy of this notebook by selecting 'File' > 'Save a copy in Drive' in the menu bars above.


Overview
=====================

While programming offers a powerful way to automate repetitive processes, it's only applicable if we are able to precisely specify and program the behavior we want. Machine learning offers us a way to avoid this need for exact specification by only requiring us to specify a goal, and then having an algorithm learn a way to achieve that goal.

The machine learning workflow typically consists of the following steps

1. Acquire and process data for the problem of interest
2. Choose a model
3. Define an objective
4. Train the model on the training data
5. Evaluate the trained model on the test data

We will walk through these steps in a simple but realistic setting.

# Setup


This lab is a Colab notebook, an interactive Python environment.
A notebook consists of multiple text blocks (like this one) and code blocks.
You can write code blocks, and then execute the code by highlighting the block then pressing control+enter. Try this with the following cell.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""
This lab relies on a number of standard scientific computing Python packages. The most important of these is NumPy (Numerical Python, commonly imported as `np`), which is a Python package for numerical computing. We'll briefly cover common numpy functions and syntax; you can find a more in-depth guide [here](https://numpy.org/doc/stable/user/quickstart.html) and you can always consult the documentation or Google if you're confused about a function.

The core object of numpy is the `array`, which is similar an ordinary Python list. Numpy arrays are frequently multi-dimensional (like a list of lists), and we call each dimension an axis.

You can create numpy arrays through a variety of ways:"""

import numpy as np

# call np.array on a valid Python list
array = np.array([[1, 2, 3], [4, 5, 6]])
print("Array 1:\n", array)

# initialize an array of all zeroes or ones
array = np.zeros((3, 4)) # where (3,4) is the shape of the array
print("Array 2:\n", array)

array = np.ones((3, 4, 5)) # where (3, 4, 5) is the shape of the array
print("Array 3:\n", array)

# initialize an array from a particular distribution
array_normal = np.random.normal(loc=0.0, scale=1.0, size=(3,4))  # uses a normal distribution with a mean at 0 and standard deviation of 1.0
print(array_normal)
#print("\nMean of array:", array_normal.mean())
#print("\nStandard deviation of array:", array_normal.std())
#print("\nMean of array along axis-0 (columns):", array_normal.mean(axis=0))

"""Unlike Python lists, numpy arrays must
* have the same number of elements along each dimension (e.g. `[[1], [2,3]]` would not be a valid numpy array).
* have all elements be of the same type

We can check the type and shape (dimensions) of an array using built-in numpy attributes of arrays.
"""

array = np.array([[[1], [2]], [[3], [4]], [[5], [6]]])
print("Data type:", array.dtype)
print("# dimensions:", array.ndim)
print("Shape/size of array:", array.shape)
print("# elements:", array.size)

"""We can access specific elements of an array using braces (like Python lists) and colons (slices)."""

array = np.arange(24).reshape(4, 6)

print("Array:\n", array)
print("Element 1, 2: ", array[1, 2]) # access one element

print("Column 2:", array[:, 1]) # access a column, using slice notation (:)
print("Row 1:", array[0]) # access a row

print("2nd-4th row, 1st-3rd col:\n", array[1:3, 0:2]) # partial rows and cols

# accessing elements using an array
print("Selecting one element from each row of array using indices in ind:")
inds = np.array([0, 2, 0, 1])
print(array[np.arange(4), inds])  # Prints "[ 0  8  12 19]"
print("-----")

"""Numpy arrays contain many useful functions as built-in methods for arrays. For example, one of the most useful is `array.sum()`. Other useful methods include `min(), max(), argmin(), argmax(), mean(), std()`. All of these methods take in an `axis` argument that specifies the dimension along which to perform the operation."""

# mathematical methods
x = np.array([[1,2],[3,4]])
print("x: \n", x)
print("-----")
print("Compute sum of all elements in x: ", np.sum(x))  # Compute sum of all elements; prints "10"
print("-----")
print("Compute sum of each column in x:", np.sum(x, axis=0))  # Compute sum of each column; prints "[4 6]"
print("-----")
print("Compute sum of each row in x: ", np.sum(x, axis=1))  # Compute sum of each row; prints "[3 7]"
print("-----")

"""We can transform arrays using Python built-in math operators, as well as more complex math operators in numpy, such as `np.power(), np.exp(), np.sqrt(), np.cos()`, etc."""

# mathematical transformations
array = np.arange(9).reshape((3,3))
print("Array 1:\n", array)

print(array + 3) # add a constant to every value, broadcasting
array[0] *= 2 # add a constant to only one row
print(array)
print(np.power(array, 2)) # square every element

"""We can also transform the dimensions and shape of arrays."""

# reshaping
array = np.arange(9)
print("Array 0:\n", array)
print("Array 0 shape:", array.shape)
array = array.reshape(3, 3)
print("Array 0:\n", array)
print("Array 0 shape:", array.shape)
print()

array1 = np.arange(9).reshape((3,3))
array2 = np.arange(9, 18).reshape((3, 3))
print("Array 1:\n", array1)
print("Array 2:\n", array2)
stack_h = np.hstack((array1, array2))
stack_v = np.vstack((array1, array2))
print("Horizontally stacked array 1 and array 2:\n", stack_h)
print("Vertically stacked array 1 and array 2:\n", stack_v)

"""We can do all the standard matrix operations using these numpy arrays. Here we show addition, transposing a matrix, matrix multiplication, and matrix inversion."""

A = np.random.randint(10, size=(3, 4))
B = np.random.randint(10, size=(3, 4))
print("Matrix A:\n", A)
print("\nMatrix B:\n", B)
print("\nA+B:\n", A+B)
print("\nTranspose B:\n", B.transpose())
print("\nShape of transpose(B):", B.transpose().shape)
print("\nMatrix multiplication of A and transpose(B):\n", np.matmul(A,B.transpose()))

C = np.matmul(A,B.transpose())
print("\n Invert AB:", np.linalg.inv(C))

"""**(exercises)** 


1. Write a function that converts an array of Celsius temperatures into an array of Farenheit temperatures (according to the formula $C = (F - 32) / 1.8$.

2. Write a function that sets the first row of a matrix to zero.

3. Create a 5x5 array of the numbers from 1-25, and then append a row of zeroes to the end.

4. Compute the product of $A$ and $B$, where $A$ is a 2x2 matrix of the first four even numbers squared and $B$ is a 2x2 matrix of the first four odd numbers squared.

5. Write a function that divides each column of a matrix by its standard deviation.


"""

def C_To_F(C_array):
  return (C_array - 32) / 1.8

def first_row_zero(matrix):
  for i in matrix[0]:
    i = 0
  return matrix

array = np.arange(1,26).reshape((5,5))


A1 = np.array([2,4,6,8])
A = np.power(A1, 2)

B1 = np.array([1,3,5,7])
B = np.power(B1, 2)

print(A*B)

"""This lab also relies on the following libraries:
* MatPlotLib is a plotting package used to create graphs and figures in order to visualize data and models
* Scikit-learn is a high-level machine learning library containing common datasets, models, and transformations for machine learning
"""

import matplotlib.pyplot as plt

"""# Regression

Regression is the general problem of predicting the value of some quantity of interest given some inputs. Many real-world problems are regression problems, such as predicting
* how much money a movie will make
* the score of a basketball game
* the number of COVID cases

More formally, we describe the regression problem as learning a relationship, or function $f$, between a vector of **input features $x$** and an **output value $y$**. When we evaluate $f$ on a new input $x$, we want the model's prediction $f(x)$ to be close to the actual output $y$.

## Data

Before we discuss what the function $f$ looks like, we will first take a look at the data, which are the ($x$, $y$) pairs. For regression, the output value $y$ will be a scalar number. The input $x$ is a vector, each entry of which we call an **input feature**. An input feature is some property of the underlying problem that we believe might be predictive of the output. For example, if we are interested in predicting how well someone will score on a test, we might use the number of hours slept the night before as a feature. Though we don't know the exact mathematical relationship between an input feature and the output, we hope that our machine learning methods can learn a function that's close to the true relationship.

For this lab, we'll use a dataset of neighborhood features and try to predict the average house value (in units of 100K) in that neighborhood, though the methods used here will be applicable to any regression dataset you're interested in. We'll load this dataset using Scikit-learn; you can read about it [here](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).
"""

from sklearn.datasets import fetch_california_housing
X, y = fetch_california_housing(return_X_y=True)

"""In using machine learning methods, it's important that we first understand the data we're using to make sure the methods we're using are appropriate for the task at hand.

Let's first understand how much data we have and what a few examples from the dataset look like. Write code to do the following: **(exercise)**
* Print the first 10 examples
* Determine the number of data points we have
* Print the min, max, and median values for a feature.
"""

print(X[:10])
print(y[:10])
print("There are ", X.shape[0] ," data points")

"""Next, let's plot the data to get a sense of what it looks like.
Since the input consists of multiple features, we'll pick out one feature at a time and see what it looks like compared to the housing price. 

**(exercise)** Use the following code to write a function that takes in the index of a feature to look at, and generates the plot for that feature.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.scatter(X[:,0], y)
plt.show()

def plot_feature(index):
  plt.scatter(X[:,index], y)
  plt.ylabel("Average housing price in 100k")
  plt.xlabel("Feature value")
  plt.show()




plot_feature(3)

"""Though we will proceed with using this dataset as provided to us, we can apply any transformations to the data that we think will help surface useful signal to our machine learning model. There is a lot of freedom and room for domain expertise in designing input features for any particular task.

If you have time, try adding a feature to the dataset by transforming one of the other features (e.g. take log, create a binary feature if a value crosses a certain threshold, etc.).
"""



"""
## Model

Now that we have looked at our data, we need to pick a type of model to learn from our data.
For this lab, we will use a simple linear model, where we assume that the relationship between the output variable and each input feature is linear. 

Mathematically, we write this model as
$$ y = f(x) = b + \sum_{i=1}^m w_i x_i $$
where $i$ indexes each dimension in the $m$-dimensional input feature vector $x$.
Together, the $w_i$ and $b$ are called the **weights** or **parameters** of the model.
If $x$ is 1-dimensional, then we get $y = wx + b$; if $x$ is 2-dimensional, then we get $y = w_1 x_1 + w_2 x_2 + b$; and so forth. 
Alternatively, we can write this as 
$$ y = w \cdot x $$
where $w$ is a vector representation of the $w_i$. To properly account for the bias term $b$, we append $b$ to $w$ (so that $w$ is $[w_1; w_2; \dots; w_m; b]$) and append a 1 to the input features $x$.

**(exercise)** Let's write a function which takes in some weights $w$ and $b$ and an input $x$ and outputs the $y$ computed by this model. You can use a for-loop, but we recommend using a vector product (you might find `np.dot()` useful).
"""

def linear(x, w, b):
  return np.dot(w,x) + b

assert linear([1], [10], 10) == 20
assert linear([1, 1], [1, 1], 0) == 2

"""
## Training

In order to use this model, we need to first set the value of the weights. But, we don't know what the ''right'' values are.
In order to do so, we first need to be able to say what constitutes good weights. Then, we can introduce an **objective function** that measures how good any set of weights are. Given this objective function, the best weights are the ones that maximize (or minimize) the objective function.

For regression, we'll use the **residual sum of squares** (RSS):
$$ L(w) = \sum_{d \in D} (y_d - f(x_d))^2 = \sum_{d \in D} (y_d - w \cdot x_d)^2 $$
where $D$ is our dataset of $(x ,y)$ pairs. Intuitively, this objective function looks at the distance from our model's predictions to the actual output. To pick the best $w$, then, we want to minimize this objective function.

To minimize this expression, we look at where the gradient (or derivative, in 1-d) of the objective function with respect to the weights is equal to zero and solve for $w$ at that point. Let's first compute the derivative. 

For convenience, we'll use a vector product representation of the model and a matrix representation of the data ($X$ is a matrix where each row is a data point $x_i$ and $y$ is a column vector of the corresponding $y_i$).

$$X = \begin{bmatrix} x_1 \\ x_2 \\ \dots \\ x_D \end{bmatrix}; y = \begin{bmatrix} y_1 \\ y_2 \\ \dots \\ y_D \end{bmatrix} $$

$$ L(w) = (y - Xw)^\top (y - Xw)$$
$$ = y^\top y - 2w^\top X^\top y + w^\top X^\top X w $$

Now we can compute the gradient:

$$ \frac{\delta L}{\delta w} = \frac{\delta}{\delta w} ( y^\top y - 2w^\top X^\top y + w^\top X^\top X w ) $$
$$ = -2X^\top y + 2 X^\top X w $$

Now let's solve for $w$ when the gradient is zero:

$$ -2X^\top y + 2 X^\top X w = 0 $$
$$ X^\top X w = X^\top y $$
$$ w^* = (X^\top X)^{-1} X^\top Y $$

We now have a formula for the optimal value $w^*$ of the weights given the data.

**(exercise)** Write a function that appends 1 to each data point, then write a function that computes the optimal weights according to this formula. You might find `np.transpose()`, `np.matmul()`, and `np.linalg.inv()` useful.
"""

# write code for computing the optimal regression weights for the dataset
def append_ones(Xs):
  return np.hstack((Xs, np.ones((Xs.shape[0], 1))))

def compute_weights(Xs, ys):
  X_set = np.linalg.inv(np.matmul(np.transpose(Xs), Xs))
  XY_set = np.matmul(np.transpose(Xs), ys)
  return np.matmul(X_set, XY_set)

X_new = append_ones(X)
w_star = compute_weights(X_new, y)

"""Now that we can compute the optimal weights, we can do so on our dataset. However, we don't just care about how well our model does on our data, but also any future data we might receive. In order to see how well our learned model performs on future data, we can split up our dataset into a **training set** and a **test set**. 

**(exercise)** Let's write a function to reserve 20% of the data for testing, and then compute the optimal weights on the remaining 80% of the data. 

What is the value of the bias term $b$ the model learns?
"""

# split the dataset into training and test
num_examples = len(X_new)
num_train = int(num_examples * .8)
X_train = X[:num_train]
y_train = y[:num_train]
X_test = X[num_train:]
y_test = y[num_train:]

# use above function to compute the best w on the training data
w = compute_weights(X_train, y_train)

"""
## Evaluation

Now that we have a model, let's evaluate how good our model is.
Again, we need some sense of what counts as good, so we will use the RSS to define good (lower RSS is better). However, since RSS is a sum, it will naturally be higher when we evaluate on more data, so let's instead compute the mean squared error (MSE) over examples. MSE is the following:

$$ MSE = \frac{1}{|D|} \sum_{i = 1}^{|D|} (y - w \cdot x)^2 $$


**(exercise)** Write a function to compute MSE, then compute the MSE for $w^*$ on the training data and the test data.

"""

# write code for evaluating the MSE of the model predictions versus the actual target on the test set
def mse(w, X, y):
  WX_set =  np.matmul(X, w)
  term = np.power(y - WX_set, 2)
  return np.mean(term)

print("Training set error", mse(w, X_train, y_train))
print("Test set error", mse(w, X_test, y_test))

"""A nice property of a linear model is that the weights are interpretable: We can look at the magnitude of the weights for each feature to get a sense of which features are most influential in predicting the output.

**(exercise)** Write code to look at the learned weights and determine which features the model puts the highest weight on.
"""

# write code for looking at the weights to see the importance of each feature
print(w)